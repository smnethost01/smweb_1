{"posts":[{"title":"[技术教程]在NAT服务器上用FRP服务搭建网站（Linux）","content":"我们先整个NAT服务器或者挂机宝，用来当服务器 推荐UPVPS的NAT服务器和挂机宝，详情请看：https://blog.smb5.tk/post/003/ 本文档没有一句废话！错过任何一个字均可能导致搭建失败，请知悉！ OpenFrp官网：https://www.openfrp.net/ 宝塔面板官网：https://bt.cn/new/download.html 1.注册OpenFrp账号： 2.创建隧道（宝塔面板）： 3.登录域名解析平台，把IP解析到Frp的CNAME上： 4.下载Frp [uname -a] #查看系统架构 —————————————— 架构 | 输出结果 i386 | i386, i686 amd64 | x86_64 arm_garbage | arm, armel armv7 | armv7l, armhf arm64 | aarch64, armv8l mips* | mips mips64* | mips64 —————————————— 选择 Linux 系统，然后选择正确的架构，点击按钮复制下载链接 使用下面的命令进入 /usr/local/bin目录并下载文件 [cd /usr/local/bin] 一般来说只需要使用这条命令: [wget &lt;下载地址&gt;] 如果上面的命令报错，请尝试这条 [curl -Lo &lt;下载地址&gt;] 5.安装Frp 解压压缩包 [tar -zxvf &lt;压缩包名&gt;] 重命名文件 [mv &lt;文件名&gt; frpc] 设置权限并校验文件是否有损坏 [chmod 755 frpc] [ls -ls frpc] [md5sum frpc] 6.配置Frp 在OpenFrp获取配置文件内容 vim frpc.ini 粘贴进去，然后按ESC键 [:wq] #保存文件 7.安装screen CentOS：[yum install screen] Ubuntu：[apt install screen] 8.创建screen [screen -R frpc] .启动Frp（宝塔面板）： [frpc] 9.登录宝塔： 浏览器打开已解析和绑定的域名，输入宝塔账号和密码，详细登录信息在SSH里 .安装环境 .配置网站： .创建隧道（网站通道）： .完成 ","link":"http://blog.smb5.tk/post/009/"},{"title":"[VPS推荐|VPS测评]萝卜数据：美国Cera高防云300G高级防御服务器折后月付仅$4.9","content":"萝卜数据怎么样，萝卜数据成立于2019年，香港公司主体运营，主要运作香港CN2大宽带，日本CN2、韩国CN2系列、以及美国防御线路业务。在线支持国际支付宝收款，使用美元结算。 下面为大家推荐的是Cera顶级防御套餐，每月最低仅需$4.9美元 萝卜数据是一家香港主体运作的公司，主要运作香港国际线路（100M~1Gbps带宽）、香港CN2 GIA、日本CN2、韩国CN2系列、美国CN2+高防保护的独立服务器，此外也有香港、日本、韩国、美国机房的VPS业务，有CN2线路、高防御可供选择。 美国Cera高防云300G，去程Cera顶级防御线路，回程9929线路，防御值为300G，国内访问速度比较满意，大陆方向攻击防御无敌。 下单地址：https://lbxu.com/buy/12.html 85折优惠码：lbxu85 CPU 内存 SSD 宽带/流量 折前价格 折后价格 1核 1G 20G 30Mbps/200G $5.54月 $4.7月 2核 2G 30G 30Mbps/500G $9.85/月 $8.4/月 4核 4G 30G 30Mbps/1TB $19.83/月 $16.8/月 8核 8G 80G 30Mbps/1.5TB $37.54/月 $31.9/月 美国Cera高防云测评 配置信息：4H4G30M 流媒体测试： 国内节点网速测试： 海外节点网速测试： IPerf3欧美节点网速测试： IO硬盘读写： PING测试： 全球丢包测试： 性能跑分测试： 国内三网去程测试： 国内三网回程测试： ","link":"http://blog.smb5.tk/post/008/"},{"title":"[VPS推荐]RackNerd：2022年美国独立日活动，西雅图VPS低价$12.99/年","content":"RackNerd开始为美国独立日搞VPS促销活动，本次活动仅限美国西海岸西雅图数据中心的VPS。一共促销3款VPS，低至12.99美元/年。依旧是1Gbps带宽，自带一个IPv4，Solusvm管理面板，纯SSD Raid阵列... 喜欢便宜VPS，喜欢RackNerd售后效率高的，可以试试！ 官方网站：https://www.racknerd.com 支持加密货币、信用卡、PayPal、支付宝等多种付款方式！ 仅限西雅图机房 内存 CPU SSD 流量 价格 1G 1核 15G 2T/月 $12.99 2G 1核 30G 4T/月 $19.99 3G 2核 45G 5T/月 $28.99 ","link":"http://blog.smb5.tk/post/007/"},{"title":"[VPS推荐]FriendHosting：新增大硬盘VPS/4.5折优惠，€7.6/半年，512M内存/1核/100G","content":"FriendHosting在原来SSD VPS的基础上新增了HDD系列的大硬盘VPS，不过其他参数没有变，依旧是KVM虚拟、不限制流量等。当前，全场SSD或者HDD系列的VPS都在搞4.5折优惠，最多可以购买半年时间，续费是原价。喜欢不限流量+大硬盘的可以试试看，老牌商家，相对还是靠谱的！ 官方网站：https://friendhosting.net/ 4.5折优惠码：SUMMER22 ，最多可买半年时间； 支持加密货币、信用卡、PayPal、支付宝、微信等几十种付款方式！ 最低配 - HDD VPS 内存：512M CPU：1核 硬盘：100G 流量：不限 价格：7.6欧元/半年 ","link":"http://blog.smb5.tk/post/006/"},{"title":"[VPS推荐]护盾云 – 香港CN2 美国GIA 大宽带高防服务器 八折","content":"护盾云，其相关团队已经耕耘云服务器与运维行业有7年，香港公司也运作有两年多，在云计算方面有着非常丰富的经验。 致力于对互联网云计算科技深入研发与运营的极客共同搭建而成，将云计算与网络核心技术转化为最稳定、安全、高速以及极具性价比的云服务器等产品提供给用户。 专注为个人开发者用户、中小型、大型企业用户提供壹站式核心网络云端部署服务，促使用户云端部署化简为零，轻松快捷运用云计算。 护盾云、多年云计算领域服务经验，遍布亚太地区的海量节点为业务推进提供强大助力。 优惠活动 大部分产品 八折 八折优惠码 :prorHf9u 美国特价活动区 优惠码不能用 http://www.hudunyun.cn/buy/20.html 香港活动区 优惠码不能用 http://www.hudunyun.cn/buy/19.html/ 最新活动区 特价年费/月费机器 售有日本，美国，香港机器 http://www.hudunyun.cn/buy/18.html 综合评价： 1.均衡型线路稳定性更高一些，不容易波动 2.速度型宽带质量更优秀，单线程下载能跑满，线路容易被攻击清洗绕日/绕美 3.宽带质量依次为 速度型双向CN2 &gt; 均衡型双向CIA-CN2 &gt; 速度型单向CDIA-CN2 &gt;均衡型单向CDIA-CN2 4.如果大宽带需求可优先选择CDIA，性价比会更高 5.以上两个线路，如遇到大流量攻击均会切换美国/日本防御线路，均衡型线路清洗能力更强。 ","link":"http://blog.smb5.tk/post/005/"},{"title":"[VPS推荐]FairyHosting：4G内存/1核/50gSSD/不限流量/100Mbps","content":"FairyHosting，2011年成立，爱沙尼亚公司，RIPE NCC、Microsoft SPLA成员。主要运作爱沙尼亚的虚拟主机、VPS、独立服务器、设备托管，另外也有香港机房的香港vps，而且还是不限流量的。支持比特币、信用卡、PayPal、支付宝等在内的多种付款方式 官方网站：https://fairyhosting.com/vds#honkong 香港VPS，不限流量 KVM虚拟，纯SSD，100Mbps带宽，不限流量，支持自定义ISO CPU 内存 SSD 价格 1核 4G 50G €9.9/月 2核 6G 100 €14.9/月 4核 8G 250 €24.9/月 6核 16G 500 €9.9/月 ","link":"http://blog.smb5.tk/post/004/"},{"title":"[NAT推荐|NAT测评]UPVPS：回馈用户推出镇江NAT服务器￥0.5/月","content":"UPVPS为回馈各位用户的支持。 特此推出镇江NAT服务器￥0.5/月~ 相当于福利了，但是不能续费，可以当月抛挂机宝用 需要NAT服务器的朋友们可以去体验一下~ 支持PayPal、支付宝、微信、QQ等在内的多种付款方式 配置如下： CPU：2核心 内存：1G 带宽：5Mbps 储存：40G IP：1个NAT IP 线路：镇江BGP 操作系统：Centos Debian Windows 价格：￥0.5 购买地址：https://idc.upvps.co/cart?fid=12&amp;gid=89 综合测试： 系统信息： 流媒体测试： CPU+RAM+IO读写测试： 国内三网回程测试： 这边可以看到带宽是可以拉满的，上传5M下载7M，也是非常实在了，日常使用CPU都是2%左右，无超开痕迹，月抛玩玩就好了，如果有想建站的小伙伴建议购买VPS和VDS ","link":"http://blog.smb5.tk/post/003/"},{"title":"[VPS推荐]Linode：新用户注册赠送$100美金免费账户余额，1核/1G内存/25G NVMe硬盘/1T流量，$5/月起，可选11个机房","content":"因为AMD出色的性能表现，目前Linode多数机房的CPU都换成了AMD EPYC 7601，此外Linode官宣支持PayPal和Google Pay付款方式。从最低赠送 $10 美金到最高赠送 $100 美金账户余额的Linode 优惠码，都在这里了，数据仅供参考！ 官网：https://www.linode.com 优惠码： 优惠码 优惠描述 LINODE10 免费 $10 USD 账户余额 DOCS10 免费 $10 USD 账户余额 PodcastInIt2017 免费 $20 USD 账户余额 bootstrapped2017 免费 $20 USD 账户余额 developertea2017 免费 $20 USD 账户余额 atp2018 免费 $20 USD 账户余额 analogue2018 免费 $20 USD 账户余额 material2018 免费 $20 USD 账户余额 clockwise2018 免费 $20 USD 账户余额 IND6060 免费 $60 USD 账户余额（仅印度用户） dw9j9mvzqn 免费 $60 USD 账户余额 5ej9s6ym9x 免费 $60 USD 账户余额 SYDNEY20 免费 $20 USD 账户余额 OBJECT20 免费 $20 USD 账户余额（庆祝 Object Storage Service 上线） 注意： 所有优惠码都只适用于 Linode 新用户 优惠套餐： 常规 VPS 套餐 内存 CPU 硬盘 流量 入带宽 出带宽 价格 1 GB 1 Core 25 GB SSD 1 TB 40 Gbps 1000 Mbps 5/mo(5 / mo (5/mo(.0075 / hr) 2 GB 1 Core 50 GB SSD 2 TB 40 Gbps 2000 Mbps 10/mo(10 / mo (10/mo(.015 / hr) 4 GB 2 Cores 80 GB SSD 4 TB 40 Gbps 4000 Mbps 20/mo(20 / mo (20/mo(.03 / hr) 8 GB 4 Cores 160 GB SSD 5 TB 40 Gbps 5000 Mbps 40/mo(40 / mo (40/mo(.06 / hr) 16 GB 6 Cores 320 GB SSD 8 TB 40 Gbps 6000 Mbps 80/mo(80 / mo (80/mo(.12 / hr) 32 GB 8 Cores 640 GB SSD 16 TB 40 Gbps 7000 Mbps 160/mo(160 / mo (160/mo(.24 / hr) 64 GB 16 Cores 1280 GB SSD 20 TB 40 Gbps 9000 Mbps 320/mo(320 / mo (320/mo(.48 / hr) 96 GB 20 Cores 1920 GB SSD 20 TB 40 Gbps 10000 Mbps 480/mo(480 / mo (480/mo(.72 / hr) 128 GB 24 Cores 2560 GB SSD 20 TB 40 Gbps 11000 Mbps 640/mo(640 / mo (640/mo(.96 / hr) 192 GB 32 Cores 3840 GB SSD 20 TB 40 Gbps 12000 Mbps $960 / mo ($1.44 / hr) 独享 CPU VPS 内存 CPU 硬盘 流量 入带宽 出带宽 价格 4 GB 2 Cores 80 GB SSD 4 TB 40 Gbps 4000 Mbps 30/mo(30 / mo (30/mo(.045 / hr) 8 GB 4 Cores 160 GB SSD 5 TB 40 Gbps 5000 Mbps 60/mo(60 / mo (60/mo(.09 / hr) 16 GB 8 Cores 320 GB SSD 6 TB 40 Gbps 6000 Mbps 120/mo(120 / mo (120/mo(.18 / hr) 32 GB 16 Cores 640 GB SSD 7 TB 40 Gbps 7000 Mbps 240/mo(240 / mo (240/mo(.36 / hr) 64 GB 32 Cores 1280 GB SSD 8 TB 40 Gbps 8000 Mbps 480/mo(480 / mo (480/mo(.72 / hr) 96 GB 48 Cores 1920 GB SSD 9 TB 40 Gbps 9000 Mbps $720 / mo ($1.08 / hr) 128 GB 50 Cores 2500 GB SSD 10 TB 40 Gbps 10000 Mbps $960 / mo ($1.44 / hr) 256 GB 56 Cores 5000 GB SSD 11 TB 40 Gbps 11000 Mbps $1920 / mo ($2.88 / hr) 512 GB 64 Cores 7200 GB SSD 12 TB 40 Gbps 12000 Mbps $3840 / mo ($5.76 / hr) 大内存 VPS 内存 CPU 硬盘 流量 入带宽 出带宽 价格 24 GB 2 Cores 20 GB SSD 5 TB 40 Gbps 5000 Mbps 60/mo(60 / mo (60/mo(.09 / hr) 48 GB 2 Cores 40 GB SSD 6 TB 40 Gbps 6000 Mbps 120/mo(120 / mo (120/mo(.18 / hr) 90 GB 4 Cores 90 GB SSD 7 TB 40 Gbps 7000 Mbps 240/mo(240 / mo (240/mo(.36 / hr) 150 GB 8 Cores 200 GB SSD 8 TB 40 Gbps 8000 Mbps 480/mo(480 / mo (480/mo(.72 / hr) 300 GB 16 Cores 340 GB SSD 9 TB 40 Gbps 9000 Mbps $960 / mo ($1.44 / hr) 新手教程 注册地址：https://www.linode.com 点击右上角的 Sign Up，开始注册，填写一些相关信息即可。注册后会进入后台完善一些注册信息，请注意以下两项的填写： Promotion Code：从上表选择一个优惠最大的 网络测试： 纽瓦克（新泽西）：http://speedtest.newark.linode.com/100MB-newark.bin 亚特兰大（美国南部）：http://speedtest.atlanta.linode.com/100MB-atlanta.bin 达拉斯（美国中部）：http://speedtest.dallas.linode.com/100MB-dallas.bin 费利蒙（西海岸）：http://speedtest.fremont.linode.com/100MB-fremont.bin 加拿大（多伦多）：http://speedtest.toronto1.linode.com/100MB-toronto.bin 法兰克福（德国）：http://speedtest.frankfurt.linode.com/100MB-frankfurt.bin 伦敦（英国）：http://speedtest.london.linode.com/100MB-london.bin 新加坡（亚洲）：http://speedtest.singapore.linode.com/100MB-singapore.bin 东京（日本2）：http://speedtest.tokyo2.linode.com/100MB-tokyo2.bin 悉尼（澳大利亚）：http://speedtest.syd1.linode.com/100MB-syd1.bin 孟买（印度）: http://speedtest.mumbai1.linode.com/100MBmumbai1.bin ","link":"http://blog.smb5.tk/post/002/"},{"title":"2021.07.13 B站是这样崩的","content":"至暗时刻 2021年7月13日22:52，SRE收到大量服务和域名的接入层不可用报警，客服侧开始收到大量用户反馈B站无法使用，同时内部同学也反馈B站无法打开，甚至APP首页也无法打开。基于报警内容，SRE第一时间怀疑机房、网络、四层LB、七层SLB等基础设施出现问题，紧急发起语音会议，拉各团队相关人员开始紧急处理（为了方便理解，下述事故处理过程做了部分简化）。 初因定位 22:55&nbsp;远程在家的相关同学登陆VPN后，无法登陆内网鉴权系统（B站内部系统有统一鉴权，需要先获取登录态后才可登陆其他内部系统），导致无法打开内部系统，无法及时查看监控、日志来定位问题。 22:57&nbsp;在公司Oncall的SRE同学（无需VPN和再次登录内网鉴权系统）发现在线业务主机房七层SLB（基于OpenResty构建） CPU 100%，无法处理用户请求，其他基础设施反馈未出问题，此时已确认是接入层七层SLB故障，排除SLB以下的业务层问题。 23:07&nbsp;远程在家的同学紧急联系负责VPN和内网鉴权系统的同学后，了解可通过绿色通道登录到内网系统。 23:17&nbsp;相关同学通过绿色通道陆续登录到内网系统，开始协助处理问题，此时处理事故的核心同学（七层SLB、四层LB、CDN）全部到位。 故障止损 23:20&nbsp;SLB运维分析发现在故障时流量有突发，怀疑SLB因流量过载不可用。因主机房SLB承载全部在线业务，先Reload SLB未恢复后尝试拒绝用户流量冷重启SLB，冷重启后CPU依然100%，未恢复。 23:22&nbsp; 从用户反馈来看，多活机房服务也不可用。SLB运维分析发现多活机房SLB请求大量超时，但CPU未过载，准备重启多活机房SLB先尝试止损。 23:23&nbsp; 此时内部群里同学反馈主站服务已恢复，观察多活机房SLB监控，请求超时数量大大降低，业务成功率恢复到50%以上。此时做了多活的业务核心功能基本恢复正常，如APP推荐、APP播放、评论&amp;弹幕拉取、动态、追番、影视等。非多活服务暂未恢复。 23:25 - 23:55&nbsp;未恢复的业务暂无其他立即有效的止损预案，此时尝试恢复主机房的SLB。 我们通过Perf发现SLB CPU热点集中在Lua函数上，怀疑跟最近上线的Lua代码有关，开始尝试回滚最近上线的Lua代码。 近期SLB配合安全同学上线了自研Lua版本的WAF，怀疑CPU热点跟此有关，尝试去掉WAF后重启SLB，SLB未恢复。 SLB两周前优化了Nginx在balance_by_lua阶段的重试逻辑，避免请求重试时请求到上一次的不可用节点，此处有一个最多10次的循环逻辑，怀疑此处有性能热点，尝试回滚后重启SLB，未恢复。 SLB一周前上线灰度了对 HTTP2 协议的支持，尝试去掉 H2 协议相关的配置并重启SLB，未恢复。新建源站SLB00:00&nbsp; SLB运维尝试回滚相关配置依旧无法恢复SLB后，决定重建一组全新的SLB集群，让CDN把故障业务公网流量调度过来，通过流量隔离观察业务能否恢复。00:20&nbsp; SLB新集群初始化完成，开始配置四层LB和公网IP。01:00&nbsp; SLB新集群初始化和测试全部完成，CDN开始切量。SLB运维继续排查CPU 100%的问题，切量由业务SRE同学协助。01:18&nbsp; 直播业务流量切换到SLB新集群，直播业务恢复正常。01:40&nbsp; 主站、电商、漫画、支付等核心业务陆续切换到SLB新集群，业务恢复。01:50&nbsp; 此时在线业务基本全部恢复。恢复SLB01:00&nbsp;SLB新集群搭建完成后，在给业务切量止损的同时，SLB运维开始继续分析CPU 100%的原因。01:10 - 01:27&nbsp;使用Lua 程序分析工具跑出一份详细的火焰图数据并加以分析，发现 CPU 热点明显集中在对 lua-resty-balancer 模块的调用中，从 SLB 流量入口逻辑一直分析到底层模块调用，发现该模块内有多个函数可能存在热点。01:28 - 01:38&nbsp;选择一台SLB节点，在可能存在热点的函数内添加 debug 日志，并重启观察这些热点函数的执行结果。01:39 - 01:58&nbsp;在分析 debug 日志后，发现 lua-resty-balancer模块中的 _gcd 函数在某次执行后返回了一个预期外的值：nan，同时发现了触发诱因的条件：某个容器IP的weight=0。01:59 - 02:06&nbsp;怀疑是该 _gcd 函数触发了 jit 编译器的某个 bug，运行出错陷入死循环导致SLB CPU 100%，临时解决方案：全局关闭 jit 编译。02:07&nbsp;SLB运维修改SLB 集群的配置，关闭 jit 编译并分批重启进程，SLB CPU 全部恢复正常，可正常处理请求。同时保留了一份异常现场下的进程core文件，留作后续分析使用。02:31 - 03:50&nbsp;SLB运维修改其他SLB集群的配置，临时关闭 jit 编译，规避风险。根因定位11:40&nbsp;在线下环境成功复现出该 bug，同时发现SLB 即使关闭 jit 编译也仍然存在该问题。此时我们也进一步定位到此问题发生的诱因：在服务的某种特殊发布模式中，会出现容器实例权重为0的情况。12:30&nbsp;经过内部讨论，我们认为该问题并未彻底解决，SLB 仍然存在极大风险，为了避免问题的再次产生，最终决定：平台禁止此发布模式；SLB 先忽略注册中心返回的权重，强制指定权重。13:24&nbsp;发布平台禁止此发布模式。14:06&nbsp;SLB 修改Lua代码忽略注册中心返回的权重。14:30&nbsp;SLB 在UAT环境发版升级，并多次验证节点权重符合预期，此问题不再产生。15:00 - 20:00&nbsp;生产所有 SLB 集群逐渐灰度并全量升级完成。原因说明背景B站在19年9月份从Tengine迁移到了OpenResty，基于其丰富的Lua能力开发了一个服务发现模块，从我们自研的注册中心同步服务注册信息到Nginx共享内存中，SLB在请求转发时，通过Lua从共享内存中选择节点处理请求，用到了OpenResty的lua-resty-balancer模块。到发生故障时已稳定运行快两年时间。在故障发生的前两个月，有业务提出想通过服务在注册中心的权重变更来实现SLB的动态调权，从而实现更精细的灰度能力。SLB团队评估了此需求后认为可以支持，开发完成后灰度上线。诱因在某种发布模式中，应用的实例权重会短暂的调整为0，此时注册中心返回给SLB的权重是字符串类型的\"0\"。此发布模式只有生产环境会用到，同时使用的频率极低，在SLB前期灰度过程中未触发此问题。SLB 在balance_by_lua阶段，会将共享内存中保存的服务IP、Port、Weight 作为参数传给lua-resty-balancer模块用于选择upstream server，在节点 weight = \"0\" 时，balancer 模块中的 _gcd 函数收到的入参 b 可能为 \"0\"。根因Lua 是动态类型语言，常用习惯里变量不需要定义类型，只需要为变量赋值即可。Lua在对一个数字字符串进行算术操作时，会尝试将这个数字字符串转成一个数字。在 Lua 语言中，如果执行数学运算 n % 0，则结果会变为 nan（Not A Number）。_gcd函数对入参没有做类型校验，允许参数b传入：\"0\"。同时因为\"0\" != 0，所以此函数第一次执行后返回是 _gcd(\"0\",nan)。如果传入的是int 0，则会触发[ if b == 0 ]分支逻辑判断，不会死循环。_gcd(\"0\",nan)函数再次执行时返回值是 _gcd(nan,nan)，然后Nginx worker开始陷入死循环，进程 CPU 100%。 问题分析1. 为何故障刚发生时无法登陆内网后台？事后复盘发现，用户在登录内网鉴权系统时，鉴权系统会跳转到多个域名下种登录的Cookie，其中一个域名是由故障的SLB代理的，受SLB故障影响当时此域名无法处理请求，导致用户登录失败。流程如下： ![](http://blog.smb5.tk/post-images/1658153219458.webp) 事后我们梳理了办公网系统的访问链路，跟用户链路隔离开，办公网链路不再依赖用户访问链路。2. 为何多活SLB在故障开始阶段也不可用？多活SLB在故障时因CDN流量回源重试和用户重试，流量突增4倍以上，连接数突增100倍到1000W级别，导致这组SLB过载。后因流量下降和重启，逐渐恢复。此SLB集群日常晚高峰CPU使用率30%左右，剩余Buffer不足两倍。如果多活SLB容量充足，理论上可承载住突发流量， 多活业务可立即恢复正常。此处也可以看到，在发生机房级别故障时，多活是业务容灾止损最快的方案，这也是故障后我们重点投入治理的一个方向。 ![](http://blog.smb5.tk/post-images/1658153279267.webp) 3. 为何在回滚SLB变更无效后才选择新建源站切量，而不是并行？我们的SLB团队规模较小，当时只有一位平台开发和一位组件运维。在出现故障时，虽有其他同学协助，但SLB组件的核心变更需要组件运维同学执行或review，所以无法并行。4. 为何新建源站切流耗时这么久？我们的公网架构如下：此处涉及三个团队：SLB团队：选择SLB机器、SLB机器初始化、SLB配置初始化四层LB团队：SLB四层LB公网IP配置CDN团队：CDN更新回源公网IP、CDN切量SLB的预案中只演练过SLB机器初始化、配置初始化，但和四层LB公网IP配置、CDN之间的协作并没有做过全链路演练，元信息在平台之间也没有联动，比如四层LB的Real Server信息提供、公网运营商线路、CDN回源IP的更新等。所以一次完整的新建源站耗时非常久。在事故后这一块的联动和自动化也是我们的重点优化方向，目前一次新集群创建、初始化、四层LB公网IP配置已经能优化到5分钟以内。5. 后续根因定位后证明关闭jit编译并没有解决问题，那当晚故障的SLB是如何恢复的？当晚已定位到诱因是某个容器IP的weight=\"0\"。此应用在1:45时发布完成，weight=\"0\"的诱因已消除。所以后续关闭jit虽然无效，但因为诱因消失，所以重启SLB后恢复正常。如果当时诱因未消失，SLB关闭jit编译后未恢复，基于定位到的诱因信息：某个容器IP的weight=0，也能定位到此服务和其发布模式，快速定位根因。优化改进此事故不管是技术侧还是管理侧都有很多优化改进。此处我们只列举当时制定的技术侧核心优化改进方向。1. 多活建设在23:23时，做了多活的业务核心功能基本恢复正常，如APP推荐、APP播放、评论&amp;弹幕拉取、动态、追番、影视等。故障时直播业务也做了多活，但当晚没及时恢复的原因是：直播移动端首页接口虽然实现了多活，但没配置多机房调度。导致在主机房SLB不可用时直播APP首页一直打不开，非常可惜。通过这次事故，我们发现了多活架构存在的一些严重问题：多活基架能力不足机房与业务多活定位关系混乱。CDN多机房流量调度不支持用户属性固定路由和分片。业务多活架构不支持写，写功能当时未恢复。部分存储组件多活同步和切换能力不足，无法实现多活。业务多活元信息缺乏平台管理哪个业务做了多活？业务是什么类型的多活，同城双活还是异地单元化？业务哪些URL规则支持多活，目前多活流量调度策略是什么？上述信息当时只能用文档临时维护，没有平台统一管理和编排。多活切量容灾能力薄弱多活切量依赖CDN同学执行，其他人员无权限，效率低。无切量管理平台，整个切量过程不可视。接入层、存储层切量分离，切量不可编排。无业务多活元信息，切量准确率和容灾效果差。我们之前的多活切量经常是这么一个场景：业务A故障了，要切量到多活机房。SRE跟研发沟通后确认要切域名A+URL A，告知CDN运维。CDN运维切量后研发发现还有个URL没切，再重复一遍上面的流程，所以导致效率极低，容灾效果也很差。&nbsp;所以我们多活建设的主要方向：多活基架能力建设优化多活基础组件的支持能力，如数据层同步组件优化、接入层支持基于用户分片，让业务的多活接入成本更低。重新梳理各机房在多活架构下的定位，梳理Czone、Gzone、Rzone业务域。推动不支持多活的核心业务和已实现多活但架构不规范的业务改造优化。多活管控能力提升统一管控所有多活业务的元信息、路由规则，联动其他平台，成为多活的元数据中心。支持多活接入层规则编排、数据层编排、预案编排、流量编排等，接入流程实现自动化和可视化。抽象多活切量能力，对接CDN、存储等组件，实现一键全链路切量，提升效率和准确率。支持多活切量时的前置能力预检，切量中风险巡检和核心指标的可观测。&nbsp;2. SLB治理架构治理故障前一个机房内一套SLB统一对外提供代理服务，导致故障域无法隔离。后续SLB需按业务部门拆分集群，核心业务部门独立SLB集群和公网IP。跟CDN团队、四层LB&amp;网络团队一起讨论确定SLB集群和公网IP隔离的管理方案。明确SLB能力边界，非SLB必备能力，统一下沉到API Gateway，SLB组件和平台均不再支持，如动态权重的灰度能力。运维能力SLB管理平台实现Lua代码版本化管理，平台支持版本升级和快速回滚。SLB节点的环境和配置初始化托管到平台，联动四层LB的API，在SLB平台上实现四层LB申请、公网IP申请、节点上线等操作，做到全流程初始化5分钟以内。SLB作为核心服务中的核心，在目前没有弹性扩容的能力下，30%的使用率较高，需要扩容把CPU降低到15%左右。优化CDN回源超时时间，降低SLB在极端故障场景下连接数。同时对连接数做极限性能压测。自研能力运维团队做项目有个弊端，开发完成自测没问题后就开始灰度上线，没有专业的测试团队介入。此组件太过核心，需要引入基础组件测试团队，对SLB输入参数做完整的异常测试。跟社区一起，Review使用到的OpenResty核心开源库源代码，消除其他风险。基于Lua已有特性和缺陷，提升我们Lua代码的鲁棒性，比如变量类型判断、强制转换等。招专业做LB的人。我们选择基于Lua开发是因为Lua简单易上手，社区有类似成功案例。团队并没有资深做Nginx组件开发的同学，也没有做C/C++开发的同学。3. 故障演练本次事故中，业务多活流量调度、新建源站速度、CDN切量速度&amp;回源超时机制均不符合预期。所以后续要探索机房级别的故障演练方案：模拟CDN回源单机房故障，跟业务研发和测试一起，通过双端上的业务真实表现来验收多活业务的容灾效果，提前优化业务多活不符合预期的隐患。灰度特定用户流量到演练的CDN节点，在CDN节点模拟源站故障，观察CDN和源站的容灾效果。模拟单机房故障，通过多活管控平台，演练业务的多活切量止损预案。&nbsp;4. 应急响应B站一直没有NOC/技术支持团队，在出现紧急事故时，故障响应、故障通报、故障协同都是由负责故障处理的SRE同学来承担。如果是普通事故还好，如果是重大事故，信息同步根本来不及。所以事故的应急响应机制必须优化：优化故障响应制度，明确故障中故障指挥官、故障处理人的职责，分担故障处理人的压力。事故发生时，故障处理人第一时间找backup作为故障指挥官，负责故障通报和故障协同。在团队里强制执行，让大家养成习惯。建设易用的故障通告平台，负责故障摘要信息录入和故障中进展同步。本次故障的诱因是某个服务使用了一种特殊的发布模式触发。我们的事件分析平台目前只提供了面向应用的事件查询能力，缺少面向用户、面向平台、面向组件的事件分析能力：跟监控团队协作，建设平台控制面事件上报能力，推动更多核心平台接入。SLB建设面向底层引擎的数据面事件变更上报和查询能力，比如服务注册信息变更时某个应用的IP更新、weight变化事件可在平台查询。扩展事件查询分析能力，除面向应用外，建设面向不同用户、不同团队、不同平台的事件查询分析能力，协助快速定位故障诱因。总结此次事故发生时，B站挂了迅速登上全网热搜，作为技术人员，身上的压力可想而知。事故已经发生，我们能做的就是深刻反思，吸取教训，总结经验，砥砺前行。此篇作为“713事故”系列之第一篇，向大家简要介绍了故障产生的诱因、根因、处理过程、优化改进。后续文章会详细介绍“713事故”后我们是如何执行优化落地的，敬请期待。最后，想说一句：多活的高可用容灾架构确实生效了。 ","link":"http://blog.smb5.tk/post/001/"}]}